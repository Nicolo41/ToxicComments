{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import transformers\n",
    "from tokenizers import BertWordPieceTokenizer\n",
    "\n",
    "\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chargement des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./datasets/test_filtered_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_portion=.2\n",
    "comments = df['comment_text']\n",
    "y = df[['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate', 'isToxic']] # Multilabel\n",
    "# y = df['isToxic'] # Binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(comments, y, test_size=test_portion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fonction d'encodage rapide"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cette fonction encode rapidement les textes en séquences d'entiers en utilisant un tokenizer BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fast_encode(texts, tokenizer, chunk_size=256, maxlen=512):\n",
    "    \"\"\"\n",
    "    Encoder for encoding the text into sequence of integers for BERT Input\n",
    "    \"\"\"\n",
    "    tokenizer.enable_truncation(max_length=maxlen)\n",
    "    tokenizer.enable_padding(length=maxlen)\n",
    "    all_ids = []\n",
    "    \n",
    "    for i in tqdm(range(0, len(texts), chunk_size)):\n",
    "        text_chunk = texts[i:i+chunk_size].tolist()\n",
    "        encs = tokenizer.encode_batch(text_chunk)\n",
    "        all_ids.extend([enc.ids for enc in encs])\n",
    "    \n",
    "    return np.array(all_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTO = tf.data.experimental.AUTOTUNE\n",
    "\n",
    "strategy = tf.distribute.get_strategy()\n",
    "# Configuration\n",
    "EPOCHS = 6\n",
    "BATCH_SIZE = 16 * strategy.num_replicas_in_sync\n",
    "MAX_LEN = 192"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenizer pré-entraîné BERT (DistilBERT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tokenizer(vocabulary_size=119547, model=BertWordPiece, unk_token=[UNK], sep_token=[SEP], cls_token=[CLS], pad_token=[PAD], mask_token=[MASK], clean_text=True, handle_chinese_chars=True, strip_accents=None, lowercase=False, wordpieces_prefix=##)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First load the real tokenizer\n",
    "tokenizer = transformers.DistilBertTokenizer.from_pretrained('distilbert-base-multilingual-cased')\n",
    "# Save the loaded tokenizer locally\n",
    "tokenizer.save_pretrained('.')\n",
    "# Reload it with the huggingface tokenizers library\n",
    "fast_tokenizer = BertWordPieceTokenizer('vocab.txt', lowercase=False)\n",
    "fast_tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encodage des données d'entrainement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 47/47 [00:00<00:00, 154.96it/s]\n",
      "100%|██████████| 12/12 [00:00<00:00, 165.42it/s]\n"
     ]
    }
   ],
   "source": [
    "X_train = fast_encode(X_train, fast_tokenizer, maxlen=MAX_LEN)\n",
    "X_test = fast_encode(X_test, fast_tokenizer, maxlen=MAX_LEN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Création des ensembles de données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Créez vos datasets\n",
    "train_dataset = (\n",
    "    tf.data.Dataset\n",
    "    .from_tensor_slices((X_train, y_train))\n",
    "    .repeat()\n",
    "    .shuffle(2048)\n",
    "    .batch(BATCH_SIZE)\n",
    "    .prefetch(AUTO)\n",
    ")\n",
    "\n",
    "valid_dataset = (\n",
    "    tf.data.Dataset\n",
    "    .from_tensor_slices((X_test, y_test))\n",
    "    .batch(BATCH_SIZE)\n",
    "    .cache()\n",
    "    .prefetch(AUTO)\n",
    ")\n",
    "\n",
    "test_dataset = (\n",
    "    tf.data.Dataset\n",
    "    .from_tensor_slices(X_test)\n",
    "    .batch(BATCH_SIZE)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configuration GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-15 15:46:30.853415: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-03-15 15:46:30.853500: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-03-15 15:46:30.853519: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-03-15 15:46:30.853911: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-03-15 15:46:30.853930: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2022] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2024-03-15 15:46:30.853971: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-03-15 15:46:30.853987: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 5520 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4070 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.9\n"
     ]
    }
   ],
   "source": [
    "gpu_options = tf.compat.v1.GPUOptions(per_process_gpu_memory_fraction=1.0)\n",
    "config = tf.compat.v1.ConfigProto(gpu_options=gpu_options)\n",
    "session = tf.compat.v1.Session(config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construction du modèle BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(transformer, max_len=512):\n",
    "    \"\"\"\n",
    "    function for training the BERT model\n",
    "    \"\"\"\n",
    "    input_word_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n",
    "    sequence_output = transformer(input_word_ids)[0]\n",
    "    cls_token = sequence_output[:, 0, :]\n",
    "    # out = Dense(1, activation='sigmoid')(cls_token) # binaire\n",
    "    out = Dense(7, activation='sigmoid')(cls_token) # multilabel\n",
    "    \n",
    "    model = Model(inputs=input_word_ids, outputs=out)\n",
    "    model.compile(Adam(learning_rate=1e-5), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFDistilBertModel: ['vocab_transform.bias', 'vocab_projector.bias', 'vocab_layer_norm.weight', 'vocab_transform.weight', 'vocab_layer_norm.bias']\n",
      "- This IS expected if you are initializing TFDistilBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFDistilBertModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_word_ids (InputLayer  [(None, 192)]             0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " tf_distil_bert_model_1 (TF  TFBaseModelOutput(last_   134734080 \n",
      " DistilBertModel)            hidden_state=(None, 192             \n",
      "                             , 768),                             \n",
      "                              hidden_states=None, at             \n",
      "                             tentions=None)                      \n",
      "                                                                 \n",
      " tf.__operators__.getitem_1  (None, 768)               0         \n",
      "  (SlicingOpLambda)                                              \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 7)                 5383      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 134739463 (513.99 MB)\n",
      "Trainable params: 134739463 (513.99 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "CPU times: user 1.54 s, sys: 279 ms, total: 1.82 s\n",
      "Wall time: 2.27 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "with strategy.scope():\n",
    "    transformer_layer = (\n",
    "        transformers.TFDistilBertModel\n",
    "        .from_pretrained('distilbert-base-multilingual-cased')\n",
    "    )\n",
    "    model = build_model(transformer_layer, max_len=MAX_LEN)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrainement du modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-15 15:46:45.069832: I external/local_xla/xla/service/service.cc:168] XLA service 0x7f95450cbc60 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2024-03-15 15:46:45.069874: I external/local_xla/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA GeForce RTX 4070 Laptop GPU, Compute Capability 8.9\n",
      "2024-03-15 15:46:45.076743: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2024-03-15 15:46:45.097346: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:454] Loaded cuDNN version 8904\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1710514005.159368   62801 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "749/749 [==============================] - 174s 210ms/step - loss: 0.1108 - accuracy: 0.4143 - val_loss: 0.0935 - val_accuracy: 0.0884\n",
      "Epoch 2/6\n",
      "749/749 [==============================] - 156s 209ms/step - loss: 0.0677 - accuracy: 0.4273 - val_loss: 0.0695 - val_accuracy: 0.6582\n",
      "Epoch 3/6\n",
      "749/749 [==============================] - 156s 208ms/step - loss: 0.0529 - accuracy: 0.4845 - val_loss: 0.0676 - val_accuracy: 0.7266\n",
      "Epoch 4/6\n",
      "749/749 [==============================] - 155s 207ms/step - loss: 0.0398 - accuracy: 0.3524 - val_loss: 0.0998 - val_accuracy: 0.2884\n",
      "Epoch 5/6\n",
      "749/749 [==============================] - 155s 207ms/step - loss: 0.0326 - accuracy: 0.3477 - val_loss: 0.0889 - val_accuracy: 0.0407\n",
      "Epoch 6/6\n",
      "749/749 [==============================] - 155s 207ms/step - loss: 0.0273 - accuracy: 0.3121 - val_loss: 0.0870 - val_accuracy: 0.3555\n"
     ]
    }
   ],
   "source": [
    "n_steps = X_train.shape[0] // BATCH_SIZE\n",
    "train_history = model.fit(\n",
    "    train_dataset,\n",
    "    steps_per_epoch=n_steps,\n",
    "    validation_data=valid_dataset,\n",
    "    epochs=EPOCHS\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, roc_curve, auc\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "\n",
    "def roc_auc(predictions,target):\n",
    "    '''\n",
    "    This methods returns the AUC Score when given the Predictions\n",
    "    and Labels\n",
    "    '''\n",
    "    \n",
    "    fpr, tpr, thresholds = roc_curve(target, predictions)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    return roc_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display en BERT binaire\n",
    "# def display_metrics(model, X_test, y_test):\n",
    "#     # prédictions et vraies valeurs\n",
    "#     y_pred = model.predict(X_test)\n",
    "#     y_true = y_test\n",
    "\n",
    "#     # Evaluation du modèle\n",
    "#     evaluate = model.evaluate(X_test, y_test, verbose=0)\n",
    "\n",
    "#     # Affichage de la loss\n",
    "#     loss = evaluate[0]\n",
    "#     print(f\"Loss: {loss} \")\n",
    "\n",
    "#     # Affichage de l'accuracy\n",
    "#     accuracy_score = evaluate[1]\n",
    "#     print(f\"Accuracy: {accuracy_score} \")\n",
    "\n",
    "#     # Affichage de l'aire sous la courbe ROC\n",
    "#     roc = roc_auc(y_pred, y_true)\n",
    "#     print(f\"Auc: {roc} \")\n",
    "\n",
    "#     # Arrondir les prédictions\n",
    "#     y_pred = y_pred.round()\n",
    "\n",
    "#     # Affichage de la matrice de confusion\n",
    "#     conf = confusion_matrix(y_true, y_pred)\n",
    "#     print(f\"Matrice de confusion : {conf}\")\n",
    "\n",
    "#     # Affichage de la précision\n",
    "#     precision = precision_score(y_true, y_pred)\n",
    "#     print(f\"Précision : {precision}\")\n",
    "\n",
    "#     # Affichage du rappel\n",
    "#     recall = recall_score(y_true, y_pred)\n",
    "#     print(f\"Rappel : {recall}\")\n",
    "\n",
    "#     # Affichage du score F1\n",
    "#     f1 = f1_score(y_true, y_pred)\n",
    "#     print(f\"F1 : {f1}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94/94 [==============================] - 12s 122ms/step\n",
      "Auc: 0.9447902716902857 \n",
      "Précision : 0.7748923863537323\n",
      "Rappel : 0.692822966507177\n",
      "Score F1 : 0.7212475944986192\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nico/miniconda3/envs/kinected/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/tmp/ipykernel_62554/4288942027.py:58: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  multi_metrics_score = pd.concat([multi_metrics_score, pd.DataFrame.from_dict([metrics])], ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "multi_metrics_score = pd.DataFrame(columns=['model', 'roc_auc', 'precision_score', 'recall_score', 'f1_score'])\n",
    "categories = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate', 'isToxic']\n",
    "import statistics\n",
    "\n",
    "\n",
    "def roc_auc_multilabel(y_pred, y_true):\n",
    "    # Calculer l'AUC-ROC pour chaque classe\n",
    "\n",
    "    y_pred_df = pd.DataFrame(y_pred, columns=categories)\n",
    "    y_true_df = pd.DataFrame(y_true, columns=categories)\n",
    "\n",
    "    roc_auc_list = []\n",
    "\n",
    "    for i in range(len(categories)):\n",
    "        y_true_cat = y_true_df[categories[i]]\n",
    "        y_pred_cat = y_pred_df[categories[i]]\n",
    "        roc_auc_list.append(roc_auc(y_pred_cat, y_true_cat))\n",
    "\n",
    "    return statistics.mean(roc_auc_list)\n",
    "\n",
    "\n",
    "def display_multi_metrics(model, X_test, y_test, name):\n",
    "    # prédictions et vraies valeurs\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_true = y_test\n",
    "\n",
    "    # Affichage de l'aire sous la courbe ROC\n",
    "    roc = roc_auc_multilabel(y_pred, y_true)\n",
    "    print(f\"Auc: {roc} \")\n",
    "\n",
    "    # Arrondir les prédictions\n",
    "    y_pred = y_pred.round()\n",
    "\n",
    "    # Affichage de la précision\n",
    "    precision = precision_score(y_true, y_pred, average='weighted')\n",
    "    print(f\"Précision : {precision}\")\n",
    "\n",
    "    # Affichage du rappel\n",
    "    recall = recall_score(y_true, y_pred, average='weighted')\n",
    "    print(f\"Rappel : {recall}\")\n",
    "\n",
    "    # Affichage du score F1\n",
    "    f1 = f1_score(y_true, y_pred, average='weighted')\n",
    "    print(f\"Score F1 : {f1}\")\n",
    "\n",
    "    return {\n",
    "        'model': name,\n",
    "        'roc_auc': roc,\n",
    "        'precision_score': precision,\n",
    "        'recall_score': recall,\n",
    "        'f1_score': f1\n",
    "    }\n",
    "\n",
    "# Affichage des métriques\n",
    "metrics = display_multi_metrics(model, X_test, y_test, 'BERT maultilabel model')\n",
    "\n",
    "# Enregistrement des metrics dans le dataframe\n",
    "multi_metrics_score = pd.concat([multi_metrics_score, pd.DataFrame.from_dict([metrics])], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display_metrics(model, X_test, y_test) # Affichage des métriques en BERT binaire"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kinected",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
